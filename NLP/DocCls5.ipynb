{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f866b0d9-0837-4cdf-95a2-ceba476cc82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models import TfidfModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "newsgroups_train = fetch_20newsgroups(subset='all',\n",
    "                                      categories=['alt.atheism',\n",
    "                                                  # 'comp.graphics',\n",
    "                                                  # 'comp.os.ms-windows.misc',\n",
    "                                                  'comp.sys.ibm.pc.hardware',\n",
    "                                                  # 'comp.sys.mac.hardware',\n",
    "                                                  # 'comp.windows.x',\n",
    "                                                  'misc.forsale',\n",
    "                                                  'rec.autos',\n",
    "                                                  # 'rec.motorcycles',\n",
    "                                                  'rec.sport.baseball'],\n",
    "                                      remove=(\"header\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ea923a-e84d-49dd-9ba5-e1fb8b64e90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOUN_TAG = ['NN', \"NNS\"]\n",
    "JJ_TAG = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "VB_TAG = [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "RB_TAG = [\"RB\", \"RBR\", \"RBS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39a5591a-cb5e-4401-a0c6-96c9db09f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_reg_exp = r'[-+]?(?:\\d+(?:\\.\\d*)?|\\.\\d+)(?:[eE][-+]?\\d+)?'\n",
    "special_sym = [\"(\", \")\", \":\", \"@\", \"?\", \",\", \"|\", \">\", \"<\", \"]\", \"\\'\", \"{\", \"/\", \"^\"\n",
    "               \"[\", \".\", \"``\", \"\\'\\'\", \"--\", \"!\", \"-\", \"*\", \"..\", \"$\", \"}\", \"#\", \"=\"]\n",
    "stop_words = stopwords.words('english') + special_sym\n",
    "\n",
    "\n",
    "def delete_stopword_and_lemmatize(listw):\n",
    "    res = []\n",
    "    for word in listw:\n",
    "        word = lemmatizer.lemmatize(word.lower())\n",
    "        if word not in stop_words and re.fullmatch(num_reg_exp, word) is None\\\n",
    "            and not any(sym in word for sym in special_sym):\n",
    "            res += [word]\n",
    "    return res\n",
    "\n",
    "\n",
    "def only_pos(list_doc, pos_t_cls):\n",
    "    len_docs = len(list_doc)\n",
    "    cnt_cls = len(pos_t_cls)\n",
    "    res = [[] for i in range(cnt_cls)]\n",
    "    \n",
    "    with tqdm(total=len_docs, position=0, leave=True) as pbar:\n",
    "        for idxd in range(len_docs):\n",
    "\n",
    "            pbar.set_description(f\"Doc: {idxd+1}/{len_docs}\")\n",
    "            pbar.update()\n",
    "\n",
    "            ndoc = [[] for i in range(cnt_cls)]\n",
    "            for elem in nltk.pos_tag(list_doc[idxd]):\n",
    "                for idxpos in range(cnt_cls):\n",
    "                    if elem[1] in pos_t_cls[idxpos]:\n",
    "                        ndoc[idxpos] += [elem[0]]\n",
    "                        \n",
    "            for idxpos in range(cnt_cls):\n",
    "                res[idxpos] += [ndoc[idxpos]]\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56108fb1-d426-44dc-a0ac-fbcb2b8cc437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    return train_test_split(x, newsgroups_train.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "134469fc-9d5e-454d-8bc2-e42654ffe06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(typename, dataset):\n",
    "    x_train, x_test, y_train, y_test = split_data(dataset)\n",
    "    print(typename)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=42, n_estimators=20, max_depth=20)\n",
    "    clf.fit(x_train, y_train)\n",
    "    acc_test = f1_score(y_true=y_test, y_pred=clf.predict(x_test), average=\"micro\")\n",
    "    print(f\"    Random Forest:     {round(acc_test, 3)}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(random_state=42, n_estimators=20)\n",
    "    clf.fit(x_train, y_train)\n",
    "    acc_test = f1_score(y_true=y_test, y_pred=clf.predict(x_test), average=\"micro\")\n",
    "    print(f\"    Gradient Boosting: {round(acc_test, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d0efff3-c3d0-4f70-952f-6207eb4f4928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bow_var(bow, dictn):\n",
    "    res = []\n",
    "    for i in range(len(bow)):\n",
    "        doc = [0 for j in range(len(dictn))]\n",
    "        for val in bow[i]:\n",
    "            doc[val[0]] = val[1]\n",
    "\n",
    "        res += [doc]\n",
    "    return res\n",
    "\n",
    "def get_tfidf_var(tfidf, dictn):\n",
    "    res = []\n",
    "    for i in range(len(tfidf)):\n",
    "        doc = [0 for j in range(len(dictn))]\n",
    "        for val in tfidf[i]:\n",
    "            doc[val[0]] = val[1]\n",
    "\n",
    "        res += [doc]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc89fea3-5de7-403c-a021-76971a5a32b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lsi(corpus, dictn):\n",
    "    lsi_model = models.LsiModel(corpus=corpus, num_topics=20, id2word=dictn)\n",
    "    lsi_res = []\n",
    "    for i in range(len(corpus)):\n",
    "        lsi_res += [[val[1] for val in lsi_model[corpus[i]]]]\n",
    "    return lsi_res\n",
    "\n",
    "def get_lda(corpus, dictn, alpha, bbeta):\n",
    "    lda_model = models.LdaModel(corpus=corpus, num_topics=20, id2word=dictn, passes=10, alpha=alpha, eta=bbeta)\n",
    "    lda_res = []\n",
    "    for i in range(len(corpus)):\n",
    "        lda_res += [[val[1] for val in lda_model.get_document_topics(corpus[i], minimum_probability=0.0)]]\n",
    "    return lda_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd67f64e-bded-4cfe-8960-f72fa4655a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenize_data = [delete_stopword_and_lemmatize(nltk.word_tokenize(newsgroups_train.data[i]))\n",
    "                 for i in range(len(newsgroups_train.data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db667a2-90b5-47ef-a811-8f69c042f05e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doc: 4740/4740: 100%|██████████████████████████████████████████████████████████████| 4740/4740 [01:22<00:00, 57.53it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_data_noun, tokenize_data_noun_jj, tokenize_data_noun_jj_vb, tokenize_data_noun_rb = only_pos(tokenize_data, \n",
    "                                                       [NOUN_TAG, NOUN_TAG + JJ_TAG, NOUN_TAG + JJ_TAG + VB_TAG, NOUN_TAG + RB_TAG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2a9e0eb-bc26-4ea0-8227-a310e8f3b06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dictn_noun, dictn_noun_jj, dictn_noun_jj_vb, dictn_noun_rb = corpora.Dictionary(tokenize_data_noun),\\\n",
    "    corpora.Dictionary(tokenize_data_noun_jj), corpora.Dictionary(tokenize_data_noun_jj_vb), corpora.Dictionary(tokenize_data_noun_rb)\n",
    "\n",
    "dictn_noun.filter_extremes(keep_n=1000)\n",
    "dictn_noun_jj.filter_extremes(keep_n=1000)\n",
    "dictn_noun_jj_vb.filter_extremes(keep_n=1000)\n",
    "dictn_noun_rb.filter_extremes(keep_n=1000)\n",
    "\n",
    "bow_noun = [dictn_noun.doc2bow(doc) for doc in tokenize_data_noun]\n",
    "bow_noun_jj = [dictn_noun_jj.doc2bow(doc) for doc in tokenize_data_noun_jj]\n",
    "bow_noun_jj_vb = [dictn_noun_jj_vb.doc2bow(doc) for doc in tokenize_data_noun_jj_vb]\n",
    "bow_noun_rb = [dictn_noun_rb.doc2bow(doc) for doc in tokenize_data_noun_rb]\n",
    "\n",
    "model = TfidfModel(bow_noun)\n",
    "tfidf_noun = [model[doc] for doc in bow_noun]\n",
    "\n",
    "model = TfidfModel(bow_noun_jj)\n",
    "tfidf_noun_jj = [model[doc] for doc in bow_noun_jj]\n",
    "\n",
    "model = TfidfModel(bow_noun_jj_vb)\n",
    "tfidf_noun_jj_vb = [model[doc] for doc in bow_noun_jj_vb]\n",
    "\n",
    "model = TfidfModel(bow_noun_rb)\n",
    "tfidf_noun_rb = [model[doc] for doc in bow_noun_rb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3197c60d-4c9e-4c34-9d03-a78926d76309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lsi_noun = get_lsi(tfidf_noun, dictn_noun)\n",
    "lsi_noun_jj = get_lsi(tfidf_noun_jj, dictn_noun_jj)\n",
    "lsi_noun_jj_vb = get_lsi(tfidf_noun_jj_vb, dictn_noun_jj_vb)\n",
    "lsi_noun_rb = get_lsi(tfidf_noun_rb, dictn_noun_rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a8a14262-fb30-4b8f-80aa-f87cb0117342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_noun = get_lda(bow_noun, dictn_noun, 'symmetric', None)\n",
    "lda_noun_jj = get_lda(bow_noun_jj, dictn_noun_jj, 'symmetric', None)\n",
    "lda_noun_jj_vb = get_lda(bow_noun_jj_vb, dictn_noun_jj_vb, 'symmetric', None)\n",
    "lda_noun_rb = get_lda(bow_noun_rb, dictn_noun_rb, 'symmetric', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f93e647e-859d-4aef-bece-5b6283338da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noun_ver = [get_bow_var(bow_noun, dictn_noun), get_tfidf_var(tfidf_noun, dictn_noun), lsi_noun, lda_noun]\n",
    "noun_jj_ver = [get_bow_var(bow_noun_jj, dictn_noun_jj), get_tfidf_var(tfidf_noun_jj, dictn_noun_jj), lsi_noun_jj, lda_noun_jj]\n",
    "noun_jj_vb_ver = [get_bow_var(bow_noun_jj_vb, dictn_noun_jj_vb), get_tfidf_var(tfidf_noun_jj_vb, dictn_noun_jj_vb), lsi_noun_jj_vb, lda_noun_jj_vb]\n",
    "noun_rb_ver = [get_bow_var(bow_noun_rb, dictn_noun_rb), get_tfidf_var(tfidf_noun_rb, dictn_noun_rb), lsi_noun_rb, lda_noun_rb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab4649b3-8bde-4f8c-8071-644c9b98387e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN-BoW:\n",
      "    Random Forest:     0.886\n",
      "    Gradient Boosting: 0.838\n",
      "NOUN-TFIDF:\n",
      "    Random Forest:     0.882\n",
      "    Gradient Boosting: 0.839\n",
      "NOUN-LSI:\n",
      "    Random Forest:     0.924\n",
      "    Gradient Boosting: 0.903\n",
      "NOUN-LDA:\n",
      "    Random Forest:     0.881\n",
      "    Gradient Boosting: 0.873\n",
      "--------------------------------\n",
      "NOUN_JJ-BoW:\n",
      "    Random Forest:     0.885\n",
      "    Gradient Boosting: 0.85\n",
      "NOUN_JJ-TFIDF:\n",
      "    Random Forest:     0.884\n",
      "    Gradient Boosting: 0.85\n",
      "NOUN_JJ-LSI:\n",
      "    Random Forest:     0.923\n",
      "    Gradient Boosting: 0.908\n",
      "NOUN_JJ-LDA:\n",
      "    Random Forest:     0.863\n",
      "    Gradient Boosting: 0.867\n",
      "--------------------------------\n",
      "NOUN_JJ_VB-BoW:\n",
      "    Random Forest:     0.901\n",
      "    Gradient Boosting: 0.861\n",
      "NOUN_JJ_VB-TFIDF:\n",
      "    Random Forest:     0.891\n",
      "    Gradient Boosting: 0.861\n",
      "NOUN_JJ_VB-LSI:\n",
      "    Random Forest:     0.935\n",
      "    Gradient Boosting: 0.921\n",
      "NOUN_JJ_VB-LDA:\n",
      "    Random Forest:     0.885\n",
      "    Gradient Boosting: 0.868\n",
      "--------------------------------\n",
      "NOUN_RB-BoW:\n",
      "    Random Forest:     0.897\n",
      "    Gradient Boosting: 0.833\n",
      "NOUN_RB-TFIDF:\n",
      "    Random Forest:     0.891\n",
      "    Gradient Boosting: 0.835\n",
      "NOUN_RB-LSI:\n",
      "    Random Forest:     0.912\n",
      "    Gradient Boosting: 0.906\n",
      "NOUN_RB-LDA:\n",
      "    Random Forest:     0.841\n",
      "    Gradient Boosting: 0.839\n"
     ]
    }
   ],
   "source": [
    "names = [\"BoW\", \"TFIDF\", \"LSI\", \"LDA\"]\n",
    "\n",
    "for i in range(len(noun_ver)):\n",
    "    get_res(f\"NOUN-{names[i]}:\", noun_ver[i])\n",
    "print(\"--------------------------------\")\n",
    "for i in range(len(noun_jj_ver)):\n",
    "    get_res(f\"NOUN_JJ-{names[i]}:\", noun_jj_ver[i])\n",
    "print(\"--------------------------------\")\n",
    "for i in range(len(noun_jj_vb_ver)):\n",
    "    get_res(f\"NOUN_JJ_VB-{names[i]}:\", noun_jj_vb_ver[i])\n",
    "print(\"--------------------------------\")\n",
    "for i in range(len(noun_rb_ver)):\n",
    "    get_res(f\"NOUN_RB-{names[i]}:\", noun_rb_ver[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ffd6bdc6-34d6-49dd-a749-1501980e3f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = [round(0.2 * i, 1) for i in range(1, 11)]\n",
    "bbeta = [round(0.2 * i, 1) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01858d2e-d523-4bc1-90fd-789494500316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN_JJ_VB (0.2, 0.2):\n",
      "    Random Forest:     0.893\n",
      "    Gradient Boosting: 0.882\n",
      "NOUN_JJ_VB (0.2, 0.4):\n",
      "    Random Forest:     0.893\n",
      "    Gradient Boosting: 0.884\n",
      "NOUN_JJ_VB (0.2, 0.6):\n",
      "    Random Forest:     0.888\n",
      "    Gradient Boosting: 0.887\n",
      "NOUN_JJ_VB (0.2, 0.8):\n",
      "    Random Forest:     0.886\n",
      "    Gradient Boosting: 0.887\n",
      "NOUN_JJ_VB (0.2, 1.0):\n",
      "    Random Forest:     0.874\n",
      "    Gradient Boosting: 0.877\n",
      "NOUN_JJ_VB (0.2, 1.2):\n",
      "    Random Forest:     0.874\n",
      "    Gradient Boosting: 0.868\n",
      "NOUN_JJ_VB (0.2, 1.4):\n",
      "    Random Forest:     0.901\n",
      "    Gradient Boosting: 0.907\n",
      "NOUN_JJ_VB (0.2, 1.6):\n",
      "    Random Forest:     0.88\n",
      "    Gradient Boosting: 0.895\n",
      "NOUN_JJ_VB (0.2, 1.8):\n",
      "    Random Forest:     0.89\n",
      "    Gradient Boosting: 0.885\n",
      "NOUN_JJ_VB (0.2, 2.0):\n",
      "    Random Forest:     0.907\n",
      "    Gradient Boosting: 0.898\n",
      "NOUN_JJ_VB (0.4, 0.2):\n",
      "    Random Forest:     0.879\n",
      "    Gradient Boosting: 0.886\n",
      "NOUN_JJ_VB (0.4, 0.4):\n",
      "    Random Forest:     0.899\n",
      "    Gradient Boosting: 0.884\n",
      "NOUN_JJ_VB (0.4, 0.6):\n",
      "    Random Forest:     0.892\n",
      "    Gradient Boosting: 0.886\n",
      "NOUN_JJ_VB (0.4, 0.8):\n",
      "    Random Forest:     0.893\n",
      "    Gradient Boosting: 0.887\n",
      "NOUN_JJ_VB (0.4, 1.0):\n",
      "    Random Forest:     0.889\n",
      "    Gradient Boosting: 0.892\n",
      "NOUN_JJ_VB (0.4, 1.2):\n",
      "    Random Forest:     0.89\n",
      "    Gradient Boosting: 0.884\n",
      "NOUN_JJ_VB (0.4, 1.4):\n",
      "    Random Forest:     0.9\n",
      "    Gradient Boosting: 0.891\n",
      "NOUN_JJ_VB (0.4, 1.6):\n",
      "    Random Forest:     0.901\n",
      "    Gradient Boosting: 0.895\n",
      "NOUN_JJ_VB (0.4, 1.8):\n",
      "    Random Forest:     0.89\n",
      "    Gradient Boosting: 0.893\n",
      "NOUN_JJ_VB (0.4, 2.0):\n",
      "    Random Forest:     0.915\n",
      "    Gradient Boosting: 0.898\n",
      "NOUN_JJ_VB (0.6, 0.2):\n",
      "    Random Forest:     0.899\n",
      "    Gradient Boosting: 0.902\n",
      "NOUN_JJ_VB (0.6, 0.4):\n",
      "    Random Forest:     0.91\n",
      "    Gradient Boosting: 0.888\n",
      "NOUN_JJ_VB (0.6, 0.6):\n",
      "    Random Forest:     0.907\n",
      "    Gradient Boosting: 0.897\n",
      "NOUN_JJ_VB (0.6, 0.8):\n",
      "    Random Forest:     0.89\n",
      "    Gradient Boosting: 0.89\n",
      "NOUN_JJ_VB (0.6, 1.0):\n",
      "    Random Forest:     0.906\n",
      "    Gradient Boosting: 0.904\n",
      "NOUN_JJ_VB (0.6, 1.2):\n",
      "    Random Forest:     0.879\n",
      "    Gradient Boosting: 0.864\n",
      "NOUN_JJ_VB (0.6, 1.4):\n",
      "    Random Forest:     0.909\n",
      "    Gradient Boosting: 0.902\n",
      "NOUN_JJ_VB (0.6, 1.6):\n",
      "    Random Forest:     0.896\n",
      "    Gradient Boosting: 0.895\n",
      "NOUN_JJ_VB (0.6, 1.8):\n",
      "    Random Forest:     0.911\n",
      "    Gradient Boosting: 0.902\n",
      "NOUN_JJ_VB (0.6, 2.0):\n",
      "    Random Forest:     0.896\n",
      "    Gradient Boosting: 0.903\n",
      "NOUN_JJ_VB (0.8, 0.2):\n",
      "    Random Forest:     0.9\n",
      "    Gradient Boosting: 0.893\n",
      "NOUN_JJ_VB (0.8, 0.4):\n",
      "    Random Forest:     0.896\n",
      "    Gradient Boosting: 0.898\n",
      "NOUN_JJ_VB (0.8, 0.6):\n",
      "    Random Forest:     0.882\n",
      "    Gradient Boosting: 0.89\n"
     ]
    }
   ],
   "source": [
    "for val_a in alpha:\n",
    "    for val_b in bbeta:\n",
    "        test_lda_noun_jj_vb = get_lda(bow_noun_jj_vb, dictn_noun_jj_vb, val_a, val_b)\n",
    "        get_res(f\"NOUN_JJ_VB ({val_a}, {val_b}):\", test_lda_noun_jj_vb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
