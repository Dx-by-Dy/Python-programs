\documentclass[12pt, a4paper]{article}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,bindingoffset=0mm]{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage[pdftex]{graphicx}
\usepackage{dsfont}
\usepackage{comment}
\usepackage[unicode, pdftex]{hyperref}
\usepackage{fancyhdr}
\usepackage{colortbl}

\graphicspath{{C:/Users/user/Desktop/NLP_foto}}
\newtheorem{theorem}{$\quad$ Теорема}
\setlength{\parindent}{5mm}
\linespread{1.4}


\begin{document}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}

\begin{center}
САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ\\
Направление: 01.03.02 Прикладная математика и информатика \\
ООП: Прикладная математика, фундаментальная информатика и программирование \\
Кафедра технологии программирования \\

\vspace*{2cm}
\large \textbf{ОТЧЕТ О НАУЧНО-ИССЛЕДОВАТЕЛЬСКОЙ РАБОТЕ}
\end{center}
\vspace*{2cm}

\hspace*{0.5cm}
\textbf{Tема задания:} Исследование алгоритмов лемматизации
\vspace*{1cm}

\hspace*{0.5cm}
\textbf{Выполнил:} Смирнов Алексей Артёмович, группа 21.Б02-пу 
\vspace*{1cm}

\hspace*{0.5cm}
\textbf{Руководитель научно- \\
\hspace*{1cm} исследовательской работы:} Блеканов Иван Станиславович, кандидат \\ \hspace*{1cm} технических наук, доцент, заведующий кафедрой технологии программирования
 
 
\vspace*{10cm}
\begin{center}
САНКТ-ПЕТЕРБУРГ \\
2023
\end{center}

\newpage

\renewcommand{\contentsname}{\begin{center}Содержание\end{center}}
\tableofcontents

\newpage

\fancyfoot[C]{\thepage}

\section{Введение}
\quad Данная работа посвящена изучению технологии Natural Language Processing(NLP) - одной из важнейших отраслей машинного обучения, дающую возможность обработки и создания информации на естественном языке. Мы рассмотрим лишь часть NLP, называемую лемматизацией, получим численную оценку нескольких популярных алгоритмов на реальных данных, что позволит рассмотреть данную задачу со всех сторон.

Лемматизация - процесс приведения словоформы к лемме, то есть к её нормальной форме. В русском языке при построении текста используются различные формы слова в зависимости от контекста, но в большинстве случаев идею текста можно понять заменив все слова на их нормальные формы, тем самым можно облегчить задачу компьютеру, уменьшив количество вариаций входных данных. Например все формы: \text{'ездил', 'ездила', 'ездили'} после лемматизации становятся равными \text{'ездить'}. Конечно, хоть лемматизация упрощает работу с текстом, после ее выполнения мы теряем часть информации заложенной изначально, но для некоторых задач, например таких как классификация и кластеризация документов, необязательно знать всю информацию заложенную в нем, достаточно различать лишь те признаки, которые позволяют определять объекты одного класса.

\subsection{Актуальность работы}
\quad Исследование различных алгоритмов лемматизации может помочь наиболее эффективно подбирать соответвующие методы в зависимости от конкретных задач на основе знания их преймуществ и недостатков, а также понимать наиболее важные проблемы и их решения в данной области. Технология NLP широко применяется в браузерах, например, при обработке запроса пользователя, извлекая ключевые идеи текста запроса, а также при индексировании вебстраниц, исправляя ошибки написания документов. Усовершенствование технологии NLP может привести к переходу к Natural Language Understanding(NLU), сделав возможным обработку не самого языка, а образов, которые закладывает человек при написании текста. 
Также ныне реализованные языковые модели, способные общаться с человеком, поддерживая разговоры на определенную тему, показывают практическую применимость рассматриваемых методов в области машинного обучения.

\subsection{Цель и задачи работы}
\quad Цель данной работы в описании и практической оценке популярных алгоритмов лемматизации для русского языка, сравнении их преймуществ и недостатков, а также в рассмотрении возможностей их усовершенствования с примерами фреймворков, использующихся на практике.

\section{Различные метрики символьных последовательностей}
\quad Основная идея ввода различных метрических хактеристик состоит в том, что мы хотим представить слово как элемент некоторого метрического пространства. Если мы будем знать все нормальные формы всех слов, то есть иметь словарь языка, то, в предположении, что словоформа не сильно отличается от своей нормальной формы по введеной метрике, мы сможем найти ближайший элемент из словаря, который и будет являться леммой.

Стоит отметить, что в современных алгоритмах данные метрики используются лишь как один из этапов приведения слова к лемме. Рассмотрим несколько примеров таких метрик.

\subsection{Расстояние Левенштейна}
\quad Расстояние Левенштейна - метрика, определяющая расстояние между символьными последовательностями по минимальному количеству односимвольных операций: \textbf{вставок}, \textbf{удалений} и \textbf{замены}, необходимых для приведения одной последовательности к другой. 

Для определения минимального количества операций определенных выше, необходимо вычислить значение $D(m, n)$, где $m$ - длина слова $\:W_1$, $n$ - длина слова $\:W_2$. Значение $\;D(m, n)\;$ можно вычислить по следующей реккурентной формуле:

$$
\label{2.1}
D(i, j) = 
\begin{cases} 
\max(i, j), \quad \quad \quad \quad \quad \; \; \min(i, j) = 0  \\
\min( \quad \quad \quad  \quad  \quad \quad \quad \quad \text{иначе}\\
\quad D(i, j - 1) + 1, \\
\quad D(i - 1, j) + 1, \\
\quad D(i - 1, j - 1) + (W_1[i] \: \neq \: W_2[j]) \\
\quad )
\end{cases}
\eqno(2.1)
$$

$$
(W_1[i] \: \neq \: W_2 [j]) = 
\begin{cases} 
1, \quad \text{если i-тый символ} \;\; W_1 \;\; \text{неравен j-тому символу}\;\; W_2 \\
0  \quad \; \text{иначе}
\end{cases}
$$

Из вида формулы $(\ref{2.1})$ можно получить оценку сложности по времени и памяти для  алгоритма, который принимал бы на вход слово длины $n$ и по известному словарю лемм языка, с количеством слов -  $N$, выдавал ближайшую лемму по расстоянию Левенштейна, которую и будем считать нормальной формой данного слова. Значение оценки по времени есть $O(Nna)$, где $a$ - средняя длина слова в словаре, по памяти - $O(na)$, которую можно сократить до $O(\min\{n, a\})$, так как нас не интересует способ приведения строк к равенству.

Сразу можно увидеть серьезный идейный недостаток данной метрики для задачи лемматизации, а именно то, что расстояние между короткими несовпадающими словами может быть довольно мало, из-за чего результат лемматизации отдельных коротких слов может быть сильно ошибочным, искажающим суть текста.

\subsection{Расстояние Джаро}
\quad Расстояние Джаро - метрика, определяющая расстояние между символьными последовательностями по их схожести. В отличии от расстояния Левенштейна, чем больше значение метрики, тем более строки похожи друг на друга, так как метрика основана на совпадающих символах.

Расстояние Джаро $\:d_j\:$ между двумя словами $\:W_1\;$ и $\:W_2\;$ определяется по следующей формуле:

$$
\label{2.2}
d_j = 
\begin{cases} 
0, &m = 0\\
\frac{1}{3}\left(\frac{m}{n_1} + \frac{m}{n_2} + \frac{m - t}{m}\right) \quad &\text{иначе}
\end{cases},
\eqno(2.2)
$$

где $n_1$, $n_2$ - длины слов $\:W_1\;$ и $\:W_2\;$, $m$ - количество совпадающих символов, расстояние между местами которых не превосходит $\lfloor \max\{n_1, n_2\}\,/\,2 \rfloor - 1$, $t$ - количество транспозиций, которое равно числу неупорядоченных пар совпадающих символов, чьи номера мест в соответвующих словах различны.

Из формулы $(\ref{2.2})$ получим оценки алгоритма описанного выше, где вместо расстояния Левенштейна будем использовать расстояние Джаро. Сложность по времени сопоставима с использованием расстояния Левенштейна, которая равна $O(Nna)$. Сложность по памяти равна $O(1)$.

Аналогично расстоянию Левенштейна, данная метрика может плохо справляться со словами малой длины, так как из формулы видно, что ценность совпадающего символа, для данной метрики, зависит от длин слов, из-за чего короткие слова могут иметь довольно большое значение $d_j$.

Также один из недостатков расстояния Джаро состоит в том, что слова состоящие из один и тех же символов на разных местах также могут иметь довольно большое значение $d_j \leq 2/3$, так как высокое значение $t$ штрафует не вычитанием из уже имеющегося, а отсутствием прибавки, что также может негативно сказаться на нашей метрической оценке.

\subsection{Расстояние Джаро-Винклера}
\quad Расстояние Джаро-Винклера - псевдометрика, являющаяся обобщением рассмотренного ранее расстояния Джаро, основанная на добавке в виде префиксного бонуса для префиксносовпадающих символьных последовательностей. Расстояние Джаро-Винклера $d_{jw}$ между двумя словами $\:W_1\;$ и $\:W_2\;$ определяется по следующей формуле:

$$
\label{2.3}
d_{jw} = 
\begin{cases} 
d_j, &d_j < b\\
d_j + lp(1-d_j) &d_j \geq b
\end{cases},
\eqno(2.3)
$$

где $l$ - длина общего префикса слов $\:W_1\;$ и $\:W_2\;$, $p$ - коэффициент масштабирования, $b$ - порог усиления, произведение $lp \leq 1$. Преймущество расстояния Джаро-Винклера над предыдущими метриками в том, что оно является трех-параметрическим семейством, что позволяет подбирать соответвующие параметры из практических нужд. Например в стандартной вариации принимают: $p=0.1$, $l=\min\{l, 4\}$, $b=0.7$.

Из формулы $(\ref{2.3})$ очевидны оценки сложности по времени - $O(Nna)$, и по памяти - $O(1)$.

Префиксная добавка $lp(1-d_j)$ направлена на устранение недостатков расстояния Джаро, описанных выше, так как, благодаря подсчету значения длины общего префикса $l$, короткие совпадающие слова получают большой бонус к занчению метрики из-за того, что совпадающий префикс может быть сравним с длиной всего слова. Проблема слов из совпадающих символов на одинаковых местах также частично решается параметром $l$, так как эти слова не получают префиксную добавку, хотя она, также как и параметр количества транспозиций $t$, не вычитает величину из уже имеющегося значения при малом $l$, а лишь не дает прибавку, что не защищает от потенциально высокого значения метрики в $2/3$.

Недостаток расстояния Джаро-Винклера состоит в том, что неправильно подобранный параметр $b$ могут ухудшить результаты по сравнению с расстоянием Джаро на определенных вариантах входных данных, так как большую добавку могут получить неверные варианты нормальной формы, например если нормальная форма слова имеет отличный префикс от тестируемой формы.

\subsection{Результаты практического тестирования метрик}
\quad Тестирование приведенных выше расстояний символьных последовательностей будем проводить на словах без контекста, взятых из размеченного детасета $\hyperlink{r1}{[1]}$ состоящим из 2000 предложений различной тематики. Оценивать работу алгоритма ламматизации будем на отношении правильных ответов к общему числу запросов, а также рассмотрим их соотношение по длине слов и частям речи. В качестве словаря нормальных форм используем датасет $\hyperlink{r1}{[1]}$ состоящий из 51733 уникальных слов. В силу приведенных выше оценок времени работы алгоритма для всех метрик - $O(Nna)$, тестирование будет проводиться на случайно выбранной тысячи слов, причем гарантируется, что указанная в размеченном датасете нормальная форма присутствует в словаре, а само слово не является стоп-словом.

Далее примем: $L$ - расстояние Левенштейна, $J$ - расстояние Джаро, $JW(\alpha, \beta, \gamma)$ - расстояние Джаро-Винклера с параметрами $l=\min\{l, \alpha\}$, $p=\beta$, $b=\gamma$. В итоге получены следующие результаты:

\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
\hline
$J$ & $L$ & $JW(4, 0.2, 0.7)$ & $JW(2, 0.4, 0.7)$ & $JW(1, 0.9, 0.7)$ \\ 
\hline
0.595 & 0.750 & 0.762 & 0.764 & 0.783 \\
\hline
\end{tabular}
\end{center}

Рассмотрим подробнее структуру полученных данных:

\begin{figure}[htbp]
\center{\includegraphics[width=1\textwidth]{Jaro_res.png} \\ Результаты тестирования расстояния Джаро ($J$)}
\label{ris:Jaro.png}
\end{figure}

\begin{figure}[htbp]
\center{\includegraphics[width=1\textwidth]{Jaro–Winkler_res.png} \\ Результаты тестирования расстояния Джаро-Винклера ($JW(2, 0.4, 0.7)$)}
\label{ris:Jaro-Winkler.png}
\end{figure}

\newpage

\begin{figure}[htp]
\center{\includegraphics[width=1\textwidth]{Levenshtein_res.png} \\ Результаты тестирования расстояния Левенштейна ($L$)}
\label{ris:Levenshtein.png}
\end{figure}

Из приведенных выше данных можно увидеть схожую структуру распределения верных ответов. Заметно, что точность для слов длины 3 у всех метрик примерно на одном уровне и выше среднего значения, это может быть связано с тем, что слова такой длины являются довольно уникальными в том, что нормальные формы различных слов, скорее всего, содержать совершенно разные символы из-за малости длины, что делает их легко отличимыми для всех метрик. 

Далее, на словах длины 4-7, виден спад точности на всех графиках ниже или на уровне средней точности, этот спад связан с недостатком всех метрик, который упоминался ранее, а именно то, что схожие слова малой длины получают лучшее значение метрики, чем схожие длинные слова, особенно данная проблема видна при использовании расстояния Джаро, где ,вплоть до слов длины 8, значение точности метрики не увеличивается.

Далее, на словах длины 8-12, виден подъем значения точности, так как, начиная со слов  средней длины, недостатки метрик, из-за которых проседала точность коротких слов, уже не оказывают столь сильного влияния, поэтому на данном промежутке длины слов все метрики показывают результат выше или на уровне средней точности.

И наконец, на словах длины 13-16, есть некоторые различиня в поведении метрик, например, при использовании расстояния Левенштейна точность оценки снижается сильно ниже среднего, это может быть связано с тем, что количество минимальных операций приведения формы слова к нормальной форме, на основе которого и измеряется расстояние Левенштейна, увеличивается по мере роста длины слова, из-за чего увеличивается вероятность спутать иную нормальную форму с нужной. При использовании расстояния Джаро или расстояния Джаро-Винклера значение точности на уровне среднего, кроме слов длины 14, что, скорее всего, является статистической погрешностью, из-за малой выборки.

\newpage

При рассмотрении графиков распределения верных ответов от части речи видна очень схожая структура. Значение верных ответов сильно ниже при лемматизации глаголов и прилагательных, это может быть связано с тем, что у этих частей речи наиболее разнообразны формы слова, что и влияет на качество ответов.

\section{Представление словарей в памяти}
\quad Одной из проблем алгоритмов леммализации является представление словарей в памяти компьютера. Понятно, что эффективное представление должно занимать не слишком много места, а главное чтобы для них существовали алгоритмы позволяющие уменьшать количество потенциальных нормальных форм, используя информацию из полученной формы слова. Рассмотрим несколько таких видов структур.

\subsection{Префиксное дерево}
\quad Префиксное дерево представляет собой структуру, с топологической точки зрения являющуюся направленным деревом. Данное дерево имеет единственный корень. Каждый узел дерева может иметь неогранниченое количество потомков. Каждый узел хранит список уникальных для узла ключей и потомков, соответствующих каждому ключу, а также флаг. Иногда узел хранит также дополнительное значение.

При заполнении дерева каждому слову соответсвует путь начинающийся из корня и заканчивающийся в узле с истинным флагом, ключами между узлами в данной цепочке выступают символы слова. Если слова имеют одинаковый префикс, то порожденная ими цепочка будет иметь совпадающие узлы, за счет которых и экономится память.

\begin{figure}[htp]
\center{\includegraphics[width=0.6\textwidth]{Prefix_Trie.png} \\ Пример префиксного дерева. Красным помечены узлы с истинным флагом}
\label{ris:Prefix_Trie.png}
\end{figure}

Очевидно, что по памяти данная структура не превосходит простое хранение всех элементов словаря, а при большом количестве слов может значительно сократить затраты. Также один из плюсов префиксного дерева это быстрая проверка существования слова в словаре за $O(n)$, где $n$ - длина слова.

\subsection{Детерминированный ациклический конечный автомат}
\quad Детерминированный конечный автомат(DFA) - набор из 5 элементов: $M = (Q, \Sigma, \delta, q_0, F)$, где $Q$ - конечное множество состояний, $\Sigma$ - конечное множество возможных входных символов (алфавит), $\delta: Q \times \Sigma \longrightarrow Q$ - функция перехода, $q_0 \in Q$ - начальное состояние, $F \subset Q$ - множество допускающих состояний.

Изначально автомат находится в стартовом состоянии $q_0$. Автомат считывает входные символы  
$r_1, r_2, \dots, r_n \in \Sigma$, переходя в состояние $q_1, q_2, \dots, q_n \in Q$, где $q_i = \delta(q_{i-1}, r_i)$, \\ $\; \forall \: i = \overline{1, n}$. Процесс продолжается до тех пор, пока не будет достигнут конец входного слова.

Будем говорить, что автомат $M$ допускает слово $r_1 r_2\dots r_n$, если для каждого символа определена $\delta(q_{i-1}, r_i)$ и $q_n \in F$. Множество всех слов, которые допускает автомат $M$ будем называть языком автомата $L(M)$.

Теперь не сложно заметить, что префиксное дерево является детерминированным ациклическим конечным автоматом(DAFSA), где $Q$ - множество узлов, $\Sigma$ - символы алфавита, $\delta$ - ребра дерева, $q_0$ - корневой узел, $F$ - узлы с истинным флагом, а так как префиксное дерево топологически является деревом, то оно не имеет циклов. Если к DAFSA применить алгоритм минимизации конечных автоматов, который уменьшает количество возможных состояний $Q$ и при этом множество $L(M)$ оставляет неизменным, можно добиться еще большей экономии памяти.


\begin{figure}[htp]
\center{\includegraphics[width=0.56\textwidth]{MinDAFSA.png} \\ Пример минимизации префиксного дерева}
\label{ris:MinDAFSA.png}
\end{figure}

Стоит отметить, что проверка допустимости слова, то есть проверка существования слова в словаре, также, как и в префиксных деревьях выполняется за $O(n)$, но при этом теперь невозможно хранить дополнительную информацию в узлах, из-за чего всю вспомогательную информацию нужно хранить в отдельнух структурах.

\subsection{Пример использования префиксного дерева в алгоритме лемматизации}
\quad В качетсве примера использования префиксного дерева возьмем алгоритм описанный в статье $\hyperlink{r2}{[2]}$. Сначала создаем префиксное дерево из всех словарных слов, далее, чтобы определить нормальную форму слова мы проверяем все возможные префиксы слова следующим образом: 

\begin{enumerate}
\item[1.] Если префикс оказался допустимым словом, то возвращаем его же.
\item[2.] Если префикс оказался недопустимым, то начинаем искать ближайщее допустимое слово в дереве методом обхода в ширину начиная из последней вершины, до которой дошел алгоритм, при этом, ограничивая глубину поиска половиной длины входного префикса.
\item[3.] Если при обходе в ширину не найдено ни одного допустимого слова, то возвращаем самое последнее допустимое слово встреченное при работе алгоритма (если такового нет, то возвращаем пустую строку), иначе возвращаем первое найденное допустимое слово.
\end{enumerate}

\begin{figure}[htp]
\center{\includegraphics[width=0.53\textwidth]{Prefix_diag.png} \\ Блок-схема обработки префикса слова в префиксном дереве}
\label{ris:Prefix_diag.png}
\end{figure}

Если расстояние Левенштейна между всеми результатами обработки префиксов и исходным словом больше длины исходного слова, то возвращаем исходное слово, иначе возвращаем результат с минимальным расстоянием.

В итоге временная сложность данного алгоритма составит $O(n^3)$, где $n$ - длина исходного слова, что намного лучше временных оценок для алгоритмов, использующих только метрики. Если учитывать, что длина слова в русском языке ограничена, то можно считать, что временная сложность составляет $O(1)$, и, в отличии от рассмотренных ранее алгоритмов, она не зависит от количества слов в словаре.

Протестируем данный алгоритм на всем датасете $\hyperlink{r1}{[1]}$, то есть на 14786 размеченных словах без контекста. Гарантируется, что слово не является стоп-словом. В качестве словаря используем аналогичный датасет $\hyperlink{r1}{[1]}$ из 51733 уникальных слов.

Полученная точноть лемматизации составила 0.790, что выше, чем ранее полученные результаты для метрик. Также намного меньшая оценка временной сложности делает данный алгоритм более применимым на практике. Рассмотрим структуру практического тестирования:

\begin{figure}[htp]
\center{\includegraphics[width=1\textwidth]{Prefix_Trie_alg_res.png} \\ Рузультаты тестирования алгоритма с использованием префиксного дерева}
\label{ris:Prefix_Trie_alg_res.png}
\end{figure}

Как видим, результаты в зависимости от длины слова довольно слабо разбросаны вокруг средней точности, кроме слов длины 2, 17, 18, 19. Это может быть связано с тем, что слова длиной 17-19 являются довольно редкими в русском языке, из-за чего найти их нормальную форму довольно просто, а высокая точность для слов длины 2 может быть связана с тем, что не так много слов такой длины не являются стоп-словами, из-за чего найти их нормальную форму также не сложно в силу их уникальности.

При рассмотрении графика результатов в зависимости от части речи видно, что доля верных ответов сильно ниже для прилагательных, глаголов и детерминативов, что объясняется тем, что у данных частей речи больше всего вариантов форм слова, из-за чего усложняется процесс лемматизации. 

\section{Современные алгоритмы лемматизации}
\quad В данной главе рассмотрим устройство современных алгоритмов лемматизации, отметим отличительные черты в сравнении между собой и приведенными выше алгоритмами.

\subsection{Библиотека pymorphy2}
\quad Библиотека pymorphy2 $\hyperlink{r3}{[3]}$ является морфологическим анализатором написанным на языке Python. Она построена на базе словаря OpenCorpora, состоящего из более чем 400 тысяч лексем и 5 миллионов отдельных слов. Данный словарь был обработан следующим образом:

\begin{enumerate}
\item[1.] Извлечение парадигмы из лексем.
\item[2.] Преобразование информации в цифры, если это возможно.
\item[3.] Кодировка словаря в формат структуры DAFSA.
\end{enumerate}

Парадигмой называется образец для склонения или спряжения; правила, согласно которым можно получить все формы слов в лексеме для неизменяемой части слова(стемы).

Из обработки 400 тысяч лексем было выделено порядка трех тысяч уникальных парадигм. Далее все парадигмы нумеруются и упаковываются в массив. 

Создается структура данных DAFSA, в которую помещаются не просто слова, а строки вида:
$$<\text{слово}> <\text{разделитель}> <\text{номер парадигмы}> <\text{номер формы в парадигме}>.$$

Данный способ хранения данных позволяет экономить память и упрощать алгоритмы, например, для получения всех вариантов разбора слова достаточно найти все ключи начинающиеся с
$$<\text{слово}> <\text{разделитель}>.$$

Итого имеется 4 массива для хранения суффиксов, префиксов, тегов(грамматической информации) и массив индексов парадигм, на которые затрачивается не более 5 Мб памяти. Созданная структура DAFSA для всех слов занимает примерно 7 Мб памяти.

Алгоритм разбора слова по словарю следующий:

\begin{enumerate}
\item[1.] Извлечение всех ключей из DAFSA, начинающихся на $<\text{слово}> <\text{разделитель}>$.
\item[2.] По информации о номере парадигмы и номере слова в парадигме восстанавливаем нормальную форму слова и грамматическую информацию из массивов.
\end{enumerate}

Алгоритм разбора несловарного слова следующий:

\begin{enumerate}
\item[1.] Отсечение известных и неизвестных префиксов. Если префикс известный, то он отсекается, оставшаяся часть разбирается, после чего склеивается обратно с префиксом. Если не найдено известного префикса, то pymorphy2 считает первую букву префиксом, потом первые 2, и так далее до 5 букв, проверяя наличие остатка в словаре.

\item[2.] Предсказание по суффиксу. Библиотека pymorphy2 собирает статистику по всем окончаниям до 5 символов. Данная статистика очищается от редких и непродуктивных парадигм и результат кодируется в структуре DAFSA в виде:
$$<\text{конец слова}> <\text{разделитель}><\text{продуктивность}> \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad$$$$\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad <\text{номер парадигмы}> <\text{номер формы в парадигме}>.$$
Происходит поиск наибольшего суффикса имеющегося в базе данных, на аналогии с которым может проводится разбор неизвестного слова. 

\item[3.] Сортировка результатов разбора из пункта 2(результаты разборов из пункта 1 в данной версии не сортируются). Полученные результаты сортируются по продуктивности, то есть номера парадигм упорядочены по частоте, с которой эти номера парадигм соответствуют данному окончанию для данной части речи без учета частотности по корпусу, то есть просто используется частотность из словаря.
\end{enumerate}

В итоге библиотека pymorphy2 занимает около 15 Мб памяти и имеет скорость разбора от 10 до 100 тысяч слов в секунду, точность разбора несловарных слов порядка 90\%.

\subsection{Алгоритм MyStem}
\quad Алгоритм MyStem $\hyperlink{r4}{[4]}$ является морфологическим анализатором компании Yandex, использующийся для обработки запросов пользователей и индексации вебстраниц.

В отличии от библиотеки pymorphy2, в качестве структуры хранения словарей используются префиксные деревья, хранящие инвертированные неизменяемые части и суффиксы.

Алгоритм разбора слова по словарю следующий:

\begin{enumerate}
\item[1.] Обрабатываем слово справа налево, извлекая из префиксного деревьа суффиксов все возможные суффиксы и ставим в соответсвие каждому суффиксу его стему.
\item[2.] Начинаем обрабатывать все пары стемов и суффиксов, начиная с наибольшего суффикса.
\item[3.] Если суффикс словоизменяющей модели, идентификатор которой получен из префиксного дерева стем, совпадает со значнием соответвующего суффикса стемы, то мы определили парадигму слова и можем получить всю информацию о слове.
\end{enumerate}

Алгоритм разбора несловарного слова следующий:
\begin{enumerate}
\item[1.] Обрабатываем слово аналогично словарному случаю, при этом для каждой стемы запоминаем все ветвящиеся узлы при обработке.
\item[2.] После обработки всех пар стем-суффикс начинаем поиск из ветвящихся узлов префиксного дерева стем, начиная с самого глубого, находя все возможные словоизменяющие модели, для которых совпадает соответвующий суффикс.
\item[3.] Оцениваем вероятность каждой выбранной модели с помощью машинного обучения в виде наивного байесовского классификатора, в основе которого лежит предположение, что события \text{'встретить стему слова'} и \text{'встретить суффикс слова'} являются независимыми:

$$P(scheme\:|\:word) = \frac{P(word\:|\:scheme)P(scheme)}{P(word)} = $$
$$ = \frac{P(stem\:|\:scheme)P(suffix\:|\:scheme)P(scheme)}{P(word)},$$

используя вероятности из данных проекта Национальный корпус русского языка $\hyperlink{r5}{[5]}$.

\item[4.] Наконец, все варианты сортируются по соответвующей вероятности и выбирается один или несколько вариантов разбора.

\end{enumerate}

Также в алгоритме MyStem имеется возможность разбора неизвестного слова по контексту с помощью сверточной нейронной сети MatrixNet, имеющую архитектуру Feature Pyramid Net(FPN). 

В итоге алгоритм MyStem занимает около 20 Мб памяти и имеет скорость разбора порядка 100 тысяч слов в секунду, оценка точности разбора несловарных слов без контекста порядка 95\% и с контекстом 98\%.

\newpage

\section{Заключение}

\subsection{Результаты работы}
\quad В данной работе мы исследовали различные алгоритмы использующиеся в технологии NLP при лемматизации текста. Протестировали и попытались объяснить результаты различных метрик символьных последовательностей, описав их недостатки и убедивших в них на практике. Рассмотрели различные способы хранения словарей для лемматизации в памяти и увидели их практическое применение в современных алгоритмах лемматизации, при сравнении алгоритмов в библиотеке pymorphy2 и алгоритма MyStem. В дальнейшем планируется исследовать способы контекстного разбора несловарных слов, полностью разобраться с алгоритмом MyStem, изучив работу нейронной сети MatrixNet, и перейти к следующим этапам технологии NLP.

\section{Список использованных источников}

\hypertarget{r1} $[1]$ \href{https://github.com/Dx-by-Dy/Python-programs/tree/main/NLP/Datasets}{\textcolor{blue}{Используемые датасеты для практического тестирования алгоритмов}} \\
\hypertarget{r2} $[2]$ \href{https://ieeexplore.ieee.org/document/9306652}{\textcolor{blue}{Lemmatization Algorithm Development for Bangla Natural Language Processing}} \\
\hypertarget{r3} $[3]$ \href{https://pymorphy2.readthedocs.io/en/stable/internals/dict.html}{\textcolor{blue}{Внутренее устройство библиотеки pymorphy2}} \\ 
\hypertarget{r4} $[4]$ \href{https://download.yandex.ru/company/iseg-las-vegas.pdf}{\textcolor{blue}{Принципы работы MyStem}} \\ 
\hypertarget{r5} $[5]$ \href{https://ruscorpora.ru}{\textcolor{blue}{Национальный корпус русского языка}}


\end{document}