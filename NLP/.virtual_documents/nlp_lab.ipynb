from numpy import zeros, array
from functools import lru_cache
from math import log
from tqdm import tqdm

from random import randint as rint
from random import seed
seed(42)

import pandas as pd
import pymorphy2
import time
import re


morph = pymorphy2.MorphAnalyzer()


lemma_dict = pd.read_csv('C:\\Python programs\\ru_dict_lemma.csv')
print(lemma_dict)


df_train = pd.read_csv('C:\\Python programs\\dataset_ru_word_lemma2.csv')
print(df_train)


df_stopwords = pd.read_csv('C:\\Python programs\\stopwords_ru.csv')
print(df_stopwords)


df_test_docs = pd.read_csv('C:\\Python programs\\test_docs.csv')
print(df_test_docs)








@lru_cache
def lev_dist(w1, w2):
    n, m = len(w1), len(w2)
    mat = zeros([n + 1, m + 1], int)
    mat[0, :] = array([i for i in range(m + 1)])
    mat[:, 0] = array([i for i in range(n + 1)])

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            mat[i][j] = min(mat[i - 1][j] + 1, mat[i][j - 1] + 1,
                            mat[i - 1][j - 1] + (w1[i - 1] != w2[j - 1]))
    return mat[n][m]


def lemma_lev(w):
    lemma = ""
    len_w = len(w)
    len_lemma = len_w
    for word in lemma_dict["lemma"]:
        if abs(len_w - len(word)) >= len_lemma:
            continue

        dist = lev_dist(word, w)
        if dist < len_lemma:
            if dist == 0:
                return word
            lemma = word
            len_lemma = dist

    return lemma if lemma != "" else w


print(lemma_lev("гнездился"))





@lru_cache
def lemma_dbsra(w):
    lemma = ""
    for i in range(len(w)):
        for j in range(len(w), i, -1):
            if trie.hasKey(w[i:j]) and j - i > len(lemma):
                lemma = w[i:j]
    return lemma if lemma != "" else w


print(lemma_dbsra("+"))





class Node(object):
    def __init__(self, value, child=None, key=None, end=False):
        if child is not None and key is not None:
            self.children = {key: child}
        else:
            self.children = {}
        self.value = value
        self._isEnd = end

    def __str__(self):
        return f"keys = {list(self.children.keys())}, value = {self.value}, \
end = {self._isEnd}"

    def addChild(self, value, key):
        self.children[key] = Node(value)

    def hasChild(self, key):
        if key in self.children.keys():
            return True
        return False

    def getChild(self, key):
        return self.children[key]

    def setValue(self, value):
        self.value = value

    def getValue(self):
        return self.value

    def isEnd(self):
        return self._isEnd

    def getKeys(self):
        return list(self.children.keys())

    def setEnd(self):
        self._isEnd = True


class PrefixTrie(object):
    def __init__(self, db=None):
        self.root = Node('')

        if db is not None:
            for word in db:
                self.insert(word, word)

    def insertDict(self, db_form, db_lemma):
        for i in range(len(db_form)):
            self.insert(db_form[i], db_lemma[i])

    def insert(self, key, value):
        node = self.root
        lenght_key = len(key)
        way = ""

        for i in range(lenght_key):
            char = key[i]
            way += char
            if not node.hasChild(char):
                node.addChild(way, char)
            node = node.getChild(char)
        if (not node.isEnd()):
            node.setValue(value)
            node.setEnd()

    def hasKey(self, key):
        node = self.root

        for i in range(len(key)):
            char = key[i]
            if node.hasChild(char):
                node = node.getChild(char)
            else:
                return False

        if node.isEnd():
            return True
        return False

    def largestPrefix(self, key):
        node = self.root
        lemma = ""
        lenght_key = len(key)

        for i in range(lenght_key):
            char = key[i]
            if not node.hasChild(char):
                break
            node = node.getChild(char)
            if node.isEnd():
                lemma = node.getValue()

        dif = lenght_key - len(lemma)
        if dif != 0:
            depth = 1
            stack = [node.getChild(i) for i in node.getKeys()]
            next_stack = []

            while depth <= max(dif, int(lenght_key / 2)):
                for elem in stack:
                    if elem.isEnd():
                        return elem.getValue()
                    next_stack += [elem.getChild(i) for i in elem.getKeys()]

                stack = next_stack.copy()
                next_stack = []
                depth += 1

        return lemma if lemma != "" else key

    @lru_cache
    def getLemma(self, word):
        prefix = ""
        lemma = word
        len_word = len(word)
        min_dist = len_word

        for i in range(len_word):
            prefix += word[i]
            new_lemma = self.largestPrefix(prefix)
            dist = lev_dist(new_lemma, word)

            if dist < min_dist and dist < len_word // 2:
                if dist == 0:
                    return new_lemma
                lemma = new_lemma
                min_dist = dist

        return lemma


trie = PrefixTrie(lemma_dict["lemma"].apply(str.lower))


print(trie.getLemma("сашин"))


cnt_PT = 0
cnt_PM2 = 0
cnt_DBSRA = 0
len_df = len(df_train.index)

start_time = time.time()
for word_id in range(len_df):
    word = df_train['form'][word_id].lower()
    lemma = df_train['lemma'][word_id].lower()

    # if not lemma_dict["lemma"].isin([lemma]).any():
    #    len_df -= 1
    # else:
    res = trie.getLemma(word)
    if res == lemma:
        cnt_PT += 1

time_PT = round(time.time() - start_time, 3)

start_time = time.time()
for word_id in range(len_df):
    word = df_train['form'][word_id].lower()
    lemma = df_train['lemma'][word_id].lower()
    res = morph.parse(word)[0].normal_form

    if res == lemma:
        cnt_PM2 += 1
time_PM2 = round(time.time() - start_time, 3)

start_time = time.time()
for word_id in range(len_df):
    word = df_train['form'][word_id].lower()
    lemma = df_train['lemma'][word_id].lower()
    res = lemma_dbsra(word)

    if res == lemma:
        cnt_DBSRA += 1
time_DBSRA = round(time.time() - start_time, 3)

# PrefixTrie : 0.804, time : 6.253
# PrefixTrie (если вычесть леммы не в словаре) : 0.833
# Pymorphy2 : 0.943, time : 6.152
# Pymorphy2 (если вычесть леммы не в словаре) : 0.99
# DBSRA : 0.608, time : 1.6
print(f"PrefixTrie : {round(cnt_PT / len_df, 3)}, time : {time_PT}")
print(f"Pymorphy2 : {round(cnt_PM2 / len_df, 3)}, time : {time_PM2}")
print(f"DBSRA : {round(cnt_DBSRA / len_df, 3)}, time : {time_DBSRA}")











trie_stopwords = PrefixTrie(df_stopwords['stopword'])


def TFIDFn(list_of_docs):
    """
    list_of_doc = [doc1, doc2, ...]
    doc1 = [word1, word2, ...]
    """

    count_of_docs = len(list_of_docs)
    num_reg_exp = r'[-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?'

    lemms_dict = {}
    id_lemma_dict = {}
    id_lemma = 0
    list_docs_dict = []

    with tqdm(total=count_of_docs*2, position=0, leave=True) as pbar:

        for id_doc in range(count_of_docs):
            doc_dict = {}

            pbar.set_description(f"Cycle: 1/2, Doc: {id_doc+1}/{count_of_docs}")
            pbar.update()

            for id_word in range(len(list_of_docs[id_doc])):
                word = list_of_docs[id_doc][id_word]
                lemma = morph.parse(word)[0].normal_form

                if trie_stopwords.hasKey(lemma) or trie_stopwords.hasKey(word) or \
                        re.fullmatch(num_reg_exp, word) is not None:
                    continue

                if lemma not in lemms_dict.keys():
                    lemms_dict[lemma] = [id_lemma, [id_doc]]
                    id_lemma_dict[id_lemma] = lemma
                    id_lemma += 1
                elif lemms_dict[lemma][1][-1] < id_doc:
                    lemms_dict[lemma][1] += [id_doc]

                id_this_lemma = lemms_dict[lemma][0]

                if id_this_lemma not in doc_dict.keys():
                    doc_dict[id_this_lemma] = 1
                else:
                    doc_dict[id_this_lemma] += 1

            list_docs_dict += [doc_dict]

        for id_doc in range(count_of_docs):

            pbar.set_description(f"Cycle: 2/2, Doc: {id_doc+1}/{count_of_docs}")
            pbar.update()

            count_words = len(list_docs_dict[id_doc].keys())
            norm_this_docs = 0

            for key in list_docs_dict[id_doc].keys():
                lemma = id_lemma_dict[key]
                res = list_docs_dict[id_doc][key] \
                    / count_words * log(count_of_docs / len(lemms_dict[lemma][1]))

                list_docs_dict[id_doc][key] = res
                norm_this_docs += res ** 2

            if norm_this_docs != 0:
                norm_this_docs = norm_this_docs ** 0.5
                for key in list_docs_dict[id_doc].keys():
                    list_docs_dict[id_doc][key] /= norm_this_docs

    return list_docs_dict, id_lemma  # лист документов и размерность пространства


print(TFIDFn([['приниматься', 'ись', 'ись', 'глядел'], ['ты', 'сидел', 'и', 'глядел']]))


list_docs = [list(df_train.loc[df_train['id_sen'] == id_sen, 'form']) for id_sen in range(1000)]


vect_list, dim = TFIDFn(list_docs)


print(dim)


def prod_vect(vect1, vect2):
    res = 0
    for key in vect1.keys() & vect2.keys():
        res += vect1[key] * vect2[key]
    return 1 - res


def euclide_dist(vect1, vect2):
    res = 0
    inters = vect1.keys() & vect2.keys()
    for key in vect1.keys():
        if key not in inters:
            res += vect1[key] ** 2
    for key in vect2.keys():
        if key not in inters:
            res += vect2[key] ** 2
    for key in inters:
        res += (vect1[key] - vect2[key]) ** 2

    return res ** 0.5


def dist_matrix(vect_list, metric=prod_vect):
    len_vect_list = len(vect_list)
    dist_docs = {}

    with tqdm(total=len_vect_list*(len_vect_list-1)//2, position=0, leave=True) as pbar:
        for i in range(len_vect_list):
            vect1 = vect_list[i]
            for j in range(i + 1, len_vect_list):
                vect2 = vect_list[j]
                dist_docs[(i, j)] = metric(vect1, vect2)

                pbar.update()

    return dist_docs


def save_dist_matrix(dist_matrix, path="C:\\Users\\user\\Desktop\\", name="dist_matrix"):
    file = open(path + name + ".txt", "w")
    for key in dist_matrix.keys():
        file.write(f"{key[0]}:{key[1]}:{dist_matrix[key]}\n")
    file.close()


def open_dist_matrix(path="C:\\Users\\user\\Desktop\\dist_matrix.txt"):
    file = open(path, "r")
    dist_matrix = {}

    for line in file:
        id_res = line.split(":")
        dist_matrix[(int(id_res[0]), int(id_res[1]))] = float(id_res[2])

    file.close()
    return dist_matrix


d = dist_matrix(vect_list)
cnt = 5
for key in d.keys():
    if d[key] > 0.5:
        print(list_docs[key[0]])
        print(list_docs[key[1]])
        print(d[key])
        print()


save_dist_matrix(d)


r = open_dist_matrix()
print(len(r))














def tokenize(doc):
    res = re.split(r'\W', doc.lower())

    while True:
        try:
            res.remove('')
        except ValueError:
            return res


print(tokenize(df_test_docs['text'][0]))


list_docs_test = [tokenize(df_test_docs['text'][i]) for i in range(len(df_test_docs.index))]


vect_docs_test, dim = TFIDFn(list_docs_test)


print(dim, vect_docs_test[:5])


t = dist_matrix(vect_docs_test)
save_dist_matrix(t)


cnt = 5
for key in t.keys():
    if t[key] < 0.1:
        print(df_test_docs['text'][key[0]])
        print()
        print(df_test_docs['text'][key[1]])
        print(t[key])
        print(df_test_docs['class'][key[0]], df_test_docs['class'][key[1]])
        print("\n---------------------------------------------------------------\n")

        cnt -= 1
        if cnt == 0:
            break





train_id, test_id = [], []
for i in range(len(vect_docs_test)):
    if rint(1, 5) == 1:
        test_id += [i]
    else:
        train_id += [i]


def sum_vect(vect1, vect2, metric):
    res = {}
    inters = vect1.keys() & vect2.keys()

    for key in vect1.keys():
        res[key] = vect1[key]
    for key in vect2.keys():
        if key in inters:
            res[key] += vect2[key]
        else:
            res[key] = vect2[key]

    if metric != euclide_dist:
        norm = sum(map(lambda x: x ** 2, res.values())) ** 0.5
        for key in res.keys():
            res[key] /= norm

    return res


def norm(vect):
    res = 0
    for key in vect.keys():
        res += vect[key] ** 2
    return res ** 0.5


def dev_vect_on_num(vect, num):
    res = {}
    for key in vect.keys():
        res[key] = vect[key] / num
    return res


def mKavg(vect_docs, train_id, test_id, name_classes, metric=prod_vect):
    # нужно согласовать норму с косинусной метрикой
    # изменить сложение для косинусной метрики

    cnt_classters = len(name_classes)
    classter_centers = [{} for i in range(cnt_classters)]
    classter_id_vect = {}
    name_classes = dict([(name_classes[i], i) for i in range(cnt_classters)])
    cnt_train_doc_in_class = dict([(i, 0) for i in range(cnt_classters)])

    for id_doc in train_id:
        classter_id = name_classes[df_test_docs['class'][id_doc]]
        classter_centers[classter_id] = \
            sum_vect(classter_centers[classter_id], vect_docs[id_doc], metric)
        cnt_train_doc_in_class[classter_id] += 1

    if metric == euclide_dist:
        for classter_id in range(cnt_classters):
            classter_centers[classter_id] = \
                dev_vect_on_num(classter_centers[classter_id], cnt_train_doc_in_class[classter_id])

    cnt_doc = len(vect_docs)
    changed = True
    cnt_cycle = 0

    while changed:
        changed = False
        cnt_cycle += 1
        print(cnt_cycle)
        if cnt_cycle == 21:
            break

        for i in range(cnt_classters):
            classter_id_vect[i] = []

        for id_doc in range(cnt_doc):
            doc = vect_docs[id_doc]
            min_dist = metric(doc, classter_centers[0]) ** 2
            classter = 0

            for id_classter in range(1, cnt_classters):
                dist = metric(doc, classter_centers[id_classter]) ** 2
                if dist < min_dist:
                    min_dist = dist
                    classter = id_classter

            classter_id_vect[classter] += [id_doc]

        for i in range(cnt_classters):
            new_class_center = {}
            for id_doc in classter_id_vect[i]:
                new_class_center = sum_vect(new_class_center, vect_docs[id_doc], metric)

            if metric == euclide_dist:
                new_class_center = dev_vect_on_num(new_class_center, len(classter_id_vect[i]))

            if not changed and classter_centers[i] != new_class_center:
                changed = True

            classter_centers[i] = new_class_center

    return classter_id_vect


def method_min_dist(vect_docs, train_id, test_id, name_classes, metric=prod_vect):
    cnt_classters = len(name_classes)
    classter_centers = [{} for i in range(cnt_classters)]
    classter_id_vect = {}
    name_classes = dict([(name_classes[i], i) for i in range(cnt_classters)])
    cnt_train_doc_in_class = dict([(i, 0) for i in range(cnt_classters)])

    for id_doc in train_id:
        classter_id = name_classes[df_test_docs['class'][id_doc]]
        classter_centers[classter_id] = \
            sum_vect(classter_centers[classter_id], vect_docs[id_doc], metric)
        cnt_train_doc_in_class[classter_id] += 1

    if metric == euclide_dist:
        for classter_id in range(cnt_classters):
            classter_centers[classter_id] = \
                dev_vect_on_num(classter_centers[classter_id], cnt_train_doc_in_class[classter_id])

    for i in range(cnt_classters):
        classter_id_vect[i] = []

    for id_doc in test_id:
        doc = vect_docs[id_doc]
        min_dist = metric(doc, classter_centers[0])
        classter = 0

        for id_classter in range(1, cnt_classters):
            dist = metric(doc, classter_centers[id_classter])
            if dist < min_dist:
                min_dist = dist
                classter = id_classter

        classter_id_vect[classter] += [id_doc]

    return classter_id_vect


name_classes = list(df_test_docs['class'].unique())
print(name_classes)


classters_vect = mKavg(vect_docs_test, train_id, test_id, name_classes)


classters_vect = method_min_dist(vect_docs_test, train_id, test_id, name_classes, euclide_dist)


id1, id2 = classters_vect[1][17], classters_vect[1][20]
print(df_test_docs['text'][id1])
print()
print(df_test_docs['text'][id2])
print(df_test_docs['class'][id1], df_test_docs['class'][id2])


acc = 0
den = 0
for i in range(13):
    local_acc = 0
    cnt_vect = 0
    dct = {}
    for id_doc in classters_vect[i]:
        if id_doc not in test_id:
            continue
        den += 1
        cnt_vect += 1
        target = df_test_docs['class'][id_doc]
        if target == name_classes[i]:
            acc += 1
            local_acc += 1

        if target not in dct.keys():
            dct[target] = 1
        else:
            dct[target] += 1
    print(dct)
    print(cnt_vect, name_classes[i])
    print(local_acc / cnt_vect)
    print()
print(acc / den)
# mKavg(cos): для train = 100% 0.19117647058823528
# mKavg(cos): для train = 80% 0.18668596237337193
# mKavg(euc): для train = 80% 0.6295224312590448
# min_dist(cos): для train = 80% 0.3140376266280753
# min_dist(euc): для train = 80% 0.8422575976845152



